# CaptchaCracker í–¥í›„ ê°œì„  ê³„íš

## ë¬¸ì„œ ê°œìš”

ì´ ë¬¸ì„œëŠ” ë ˆê±°ì‹œ ë§ˆì´ê·¸ë ˆì´ì…˜(v1.0.0) **ì´í›„**ì— ìˆ˜í–‰í•  ê°œì„  ì‚¬í•­ì„ ë‹¤ë£¹ë‹ˆë‹¤.
`REFACTORING_PLAN.md`ê°€ ê¸°ìˆ  ë¶€ì±„ í•´ì†Œì— ì§‘ì¤‘í•œë‹¤ë©´, ë³¸ ë¬¸ì„œëŠ” **ê¸°ëŠ¥ ë° ì„±ëŠ¥ í–¥ìƒ**ì— ì´ˆì ì„ ë§ì¶¥ë‹ˆë‹¤.

- **ì„ í–‰ ì‘ì—…**: `REFACTORING_PLAN.md` Phase 1-6 ì™„ë£Œ
- **ëŒ€ìƒ ë²„ì „**: v1.1.0 ì´ìƒ
- **ì‘ì„±ì¼**: 2025-11-19

---

## ê°œì„  ì¹´í…Œê³ ë¦¬

1. ğŸ¯ **ëª¨ë¸ ì •í™•ë„ í–¥ìƒ**
2. âš¡ **ì„±ëŠ¥ ìµœì í™”**
3. ğŸ”§ **ëª¨ë¸ ì•„í‚¤í…ì²˜ ê°œì„ **
4. ğŸ“Š **ë°ì´í„° ë° í•™ìŠµ ê°œì„ **
5. ğŸš€ **ìƒˆë¡œìš´ ê¸°ëŠ¥**
6. ğŸŒ **í”„ë¡œë•ì…˜ ì¤€ë¹„**
7. ğŸ”¬ **ì—°êµ¬ ë° ì‹¤í—˜**

---

## 1. ğŸ¯ ëª¨ë¸ ì •í™•ë„ í–¥ìƒ

### 1.1 ë°ì´í„° ì¦ê°• (Data Augmentation)

**í˜„ì¬ ìƒíƒœ**: ë°ì´í„° ì¦ê°• ì—†ìŒ

**ê°œì„  ë°©ì•ˆ**:
```python
# í•™ìŠµ ì‹œ ì‹¤ì‹œê°„ ë°ì´í„° ì¦ê°• ì ìš©
augmentation = tf.keras.Sequential([
    layers.RandomRotation(0.05),           # Â±5ë„ íšŒì „
    layers.RandomTranslation(0.1, 0.1),    # 10% ì´ë™
    layers.RandomBrightness(0.2),          # ë°ê¸° ì¡°ì ˆ
    layers.RandomContrast(0.2),            # ëŒ€ë¹„ ì¡°ì ˆ
    layers.GaussianNoise(0.01),            # ë…¸ì´ì¦ˆ ì¶”ê°€
])
```

**ì˜ˆìƒ íš¨ê³¼**:
- ê³¼ì í•©(overfitting) ê°ì†Œ
- ë‹¤ì–‘í•œ ìº¡ì±  ìŠ¤íƒ€ì¼ì— ëŒ€í•œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ
- ì˜ˆìƒ ì •í™•ë„ í–¥ìƒ: 3-5%

**ìš°ì„ ìˆœìœ„**: â­â­â­ High

---

### 1.2 Attention Mechanism ì¶”ê°€

**í˜„ì¬ ìƒíƒœ**: Bidirectional LSTMë§Œ ì‚¬ìš©

**ê°œì„  ë°©ì•ˆ**:
```python
# Attention ë ˆì´ì–´ ì¶”ê°€
attention = layers.Attention()
lstm_output = Bidirectional(LSTM(128, return_sequences=True))(x)
attention_output = attention([lstm_output, lstm_output])
```

ë˜ëŠ” Transformer ê¸°ë°˜ ì•„í‚¤í…ì²˜:
```python
# Multi-Head Attention ì‚¬ìš©
attention = layers.MultiHeadAttention(num_heads=4, key_dim=64)
attention_output = attention(query=x, key=x, value=x)
```

**ì˜ˆìƒ íš¨ê³¼**:
- ê¸´ ì‹œí€€ìŠ¤ì— ëŒ€í•œ ì¸ì‹ ì„±ëŠ¥ í–¥ìƒ
- ë¬¸ì ê°„ ê´€ê³„ í•™ìŠµ ê°œì„ 
- ì˜ˆìƒ ì •í™•ë„ í–¥ìƒ: 2-4%

**ìš°ì„ ìˆœìœ„**: â­â­â­ High

---

### 1.3 ì•™ìƒë¸” (Ensemble) ëª¨ë¸

**ê°œì„  ë°©ì•ˆ**:
```python
# ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê²°í•©
models = [
    load_model('weights_v1.h5'),
    load_model('weights_v2.h5'),
    load_model('weights_v3.h5'),
]

# Voting ë˜ëŠ” Averaging
predictions = [model.predict(image) for model in models]
final_prediction = ensemble_vote(predictions)
```

**ì˜ˆìƒ íš¨ê³¼**:
- ì˜ˆì¸¡ ì•ˆì •ì„± í–¥ìƒ
- ì˜ˆìƒ ì •í™•ë„ í–¥ìƒ: 1-3%
- ë‹¨ì : ì¶”ë¡  ì‹œê°„ ì¦ê°€ (3ë°°)

**ìš°ì„ ìˆœìœ„**: â­â­ Medium

---

### 1.4 CTC Loss ëŒ€ì•ˆ íƒìƒ‰

**í˜„ì¬ ìƒíƒœ**: CTC Loss ì‚¬ìš©

**ëŒ€ì•ˆ**:
1. **Attention-based Encoder-Decoder**
   - Seq2Seq with Attention
   - ë” ìœ ì—°í•œ ì‹œí€€ìŠ¤ ì²˜ë¦¬

2. **Transformer Decoder**
   - BERT-style masked prediction
   - ì–‘ë°©í–¥ ë¬¸ë§¥ í™œìš©

**ìš°ì„ ìˆœìœ„**: â­â­ Medium (ì—°êµ¬ í•„ìš”)

---

## 2. âš¡ ì„±ëŠ¥ ìµœì í™”

### 2.1 ëª¨ë¸ ê²½ëŸ‰í™”

#### 2.1.1 ëª¨ë¸ ì–‘ìí™” (Quantization)

**ë°©ë²•**:
```python
# INT8 ì–‘ìí™”
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
```

**ì˜ˆìƒ íš¨ê³¼**:
- ëª¨ë¸ í¬ê¸°: 75% ê°ì†Œ
- ì¶”ë¡  ì†ë„: 2-4ë°° í–¥ìƒ
- ì •í™•ë„ ì†ì‹¤: <1%

**ìš°ì„ ìˆœìœ„**: â­â­â­ High

---

#### 2.1.2 ì§€ì‹ ì¦ë¥˜ (Knowledge Distillation)

**ë°©ë²•**:
```python
# Teacher ëª¨ë¸ (í° ëª¨ë¸)ì˜ ì§€ì‹ì„ Student ëª¨ë¸ (ì‘ì€ ëª¨ë¸)ë¡œ ì „ë‹¬
teacher = load_model('large_model.h5')
student = build_small_model()

# Soft targets í•™ìŠµ
student.train_with_teacher(teacher, temperature=3.0)
```

**ì˜ˆìƒ íš¨ê³¼**:
- ëª¨ë¸ í¬ê¸°: 50% ê°ì†Œ
- ì¶”ë¡  ì†ë„: 2-3ë°° í–¥ìƒ
- ì •í™•ë„: í° ëª¨ë¸ì˜ 95%+ ìœ ì§€

**ìš°ì„ ìˆœìœ„**: â­â­ Medium

---

### 2.2 ì¶”ë¡  ìµœì í™”

#### 2.2.1 ONNX ë³€í™˜

**ë°©ë²•**:
```python
import tf2onnx

# TensorFlow â†’ ONNX ë³€í™˜
onnx_model = tf2onnx.convert.from_keras(model)

# ONNX Runtimeìœ¼ë¡œ ì¶”ë¡ 
import onnxruntime as ort
session = ort.InferenceSession(onnx_model)
```

**ì˜ˆìƒ íš¨ê³¼**:
- í¬ë¡œìŠ¤ í”Œë«í¼ í˜¸í™˜ì„±
- ì¶”ë¡  ì†ë„: 1.5-2ë°° í–¥ìƒ
- ë‹¤ì–‘í•œ í•˜ë“œì›¨ì–´ ìµœì í™” (CPU, GPU, TensorRT)

**ìš°ì„ ìˆœìœ„**: â­â­â­ High

---

#### 2.2.2 ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”

**í˜„ì¬ ìƒíƒœ**: ì´ë¯¸ì§€ë¥¼ í•˜ë‚˜ì”© ì²˜ë¦¬

**ê°œì„ **:
```python
# ë™ì  ë°°ì¹˜ í¬ê¸° ì§€ì›
def predict_batch(images, batch_size='auto'):
    if batch_size == 'auto':
        # GPU ë©”ëª¨ë¦¬ ê¸°ë°˜ ìë™ ë°°ì¹˜ í¬ê¸° ê²°ì •
        batch_size = estimate_optimal_batch_size()

    results = []
    for batch in batched(images, batch_size):
        results.extend(model.predict(batch))
    return results
```

**ì˜ˆìƒ íš¨ê³¼**:
- ëŒ€ëŸ‰ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œ 2-5ë°° ì†ë„ í–¥ìƒ

**ìš°ì„ ìˆœìœ„**: â­â­â­ High

---

#### 2.2.3 ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ìµœì í™”

**ë°©ë²•**:
```python
# TensorFlow Dataset API ìµœì í™”
dataset = tf.data.Dataset.from_tensor_slices(images)
dataset = dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)
dataset = dataset.batch(batch_size)
dataset = dataset.prefetch(tf.data.AUTOTUNE)
```

**ì˜ˆìƒ íš¨ê³¼**:
- I/O ë³‘ëª© ì œê±°
- GPU í™œìš©ë¥  í–¥ìƒ

**ìš°ì„ ìˆœìœ„**: â­â­ Medium

---

### 2.3 Mixed Precision í•™ìŠµ

**ë°©ë²•**:
```python
# Mixed Precision ì •ì±… ì„¤ì •
policy = tf.keras.mixed_precision.Policy('mixed_float16')
tf.keras.mixed_precision.set_global_policy(policy)

# í•™ìŠµ ì†ë„ í–¥ìƒ (GPU ì‚¬ìš© ì‹œ)
model.compile(optimizer='adam', loss='ctc', metrics=['accuracy'])
```

**ì˜ˆìƒ íš¨ê³¼**:
- í•™ìŠµ ì†ë„: 2-3ë°° í–¥ìƒ
- GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 50% ê°ì†Œ
- ì •í™•ë„ ì†ì‹¤: ê±°ì˜ ì—†ìŒ

**ìš°ì„ ìˆœìœ„**: â­â­â­ High

---

## 3. ğŸ”§ ëª¨ë¸ ì•„í‚¤í…ì²˜ ê°œì„ 

### 3.1 CNN Backbone ì—…ê·¸ë ˆì´ë“œ

**í˜„ì¬ ìƒíƒœ**: ë‹¨ìˆœí•œ 2-layer CNN

**ê°œì„  ì˜µì…˜**:

#### ì˜µì…˜ A: ResNet Blocks
```python
def residual_block(x, filters):
    shortcut = x
    x = Conv2D(filters, 3, padding='same')(x)
    x = BatchNormalization()(x)
    x = ReLU()(x)
    x = Conv2D(filters, 3, padding='same')(x)
    x = BatchNormalization()(x)
    x = Add()([x, shortcut])
    return ReLU()(x)
```

#### ì˜µì…˜ B: EfficientNet Backbone
```python
from tensorflow.keras.applications import EfficientNetB0

base_model = EfficientNetB0(
    include_top=False,
    weights='imagenet',  # ë˜ëŠ” None
    input_shape=(50, 200, 1)
)
```

#### ì˜µì…˜ C: Vision Transformer (ViT)
```python
# Patch embedding + Transformer encoder
patches = extract_patches(image)
encoded = TransformerEncoder()(patches)
```

**ìš°ì„ ìˆœìœ„**: â­â­ Medium

---

### 3.2 Bidirectional LSTM ëŒ€ì•ˆ

**í˜„ì¬ ìƒíƒœ**: Bidirectional LSTM 2ì¸µ

**ëŒ€ì•ˆ**:

#### ì˜µì…˜ A: GRU (ë” ê°€ë²¼ì›€)
```python
Bidirectional(GRU(128, return_sequences=True))
Bidirectional(GRU(64, return_sequences=True))
```

#### ì˜µì…˜ B: 1D Convolution
```python
Conv1D(128, 3, padding='same', activation='relu')
Conv1D(64, 3, padding='same', activation='relu')
```

#### ì˜µì…˜ C: Transformer Encoder
```python
TransformerEncoder(
    num_layers=2,
    d_model=128,
    num_heads=4,
    dff=512
)
```

**ìš°ì„ ìˆœìœ„**: â­â­ Medium

---

### 3.3 Dynamic Image Size ì§€ì›

**í˜„ì¬ ìƒíƒœ**: ê³ ì • í¬ê¸° (200x50)ë§Œ ì§€ì›

**ê°œì„ **:
```python
# ë‹¤ì–‘í•œ í¬ê¸° ì§€ì›
model = CaptchaModel.load('weights.h5', image_size='auto')

# ì¶”ë¡  ì‹œ ìë™ ë¦¬ì‚¬ì´ì§•
result = model.predict('captcha_300x60.png')  # ìë™ ì¡°ì •
```

**êµ¬í˜„ ë°©ì•ˆ**:
1. Fully Convolutional Network (FCN) ì‚¬ìš©
2. Adaptive Pooling ì‚¬ìš©
3. ì—¬ëŸ¬ í¬ê¸°ë³„ ëª¨ë¸ í•™ìŠµ

**ìš°ì„ ìˆœìœ„**: â­â­ Medium

---

## 4. ğŸ“Š ë°ì´í„° ë° í•™ìŠµ ê°œì„ 

### 4.1 í•©ì„± ë°ì´í„° ìƒì„±

**ëª©ì **: í•™ìŠµ ë°ì´í„° ë¶€ì¡± ë¬¸ì œ í•´ê²°

**ë°©ë²•**:
```python
from captcha.image import ImageCaptcha

generator = ImageCaptcha(width=200, height=50)

# ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ì˜ ìº¡ì±  ìƒì„±
for _ in range(10000):
    text = generate_random_text()
    image = generator.generate(text)
    save_image(image, f'{text}.png')
```

**ê³ ë ¤ì‚¬í•­**:
- í°íŠ¸, ìƒ‰ìƒ, ì™œê³¡ ì •ë„ ë‹¤ì–‘í™”
- ì‹¤ì œ ìº¡ì± ì™€ ìœ ì‚¬í•œ ë…¸ì´ì¦ˆ ì¶”ê°€
- GANì„ í™œìš©í•œ ê³ í’ˆì§ˆ í•©ì„± ë°ì´í„°

**ìš°ì„ ìˆœìœ„**: â­â­â­ High

---

### 4.2 í•™ìŠµ ì „ëµ ê°œì„ 

#### 4.2.1 Learning Rate Scheduler
```python
# Cosine Annealing
lr_schedule = tf.keras.optimizers.schedules.CosineDecay(
    initial_learning_rate=1e-3,
    decay_steps=1000
)

# Warm-up + Cosine Decay
lr_schedule = WarmupCosineDecay(
    warmup_steps=100,
    total_steps=1000,
    initial_lr=1e-5,
    max_lr=1e-3
)
```

#### 4.2.2 Curriculum Learning
```python
# ì‰¬ìš´ ìƒ˜í”Œë¶€í„° ì ì§„ì ìœ¼ë¡œ ì–´ë ¤ìš´ ìƒ˜í”Œ í•™ìŠµ
epoch_1_10: simple_captchas (4-5 characters, no noise)
epoch_11_30: medium_captchas (5-6 characters, light noise)
epoch_31_100: hard_captchas (6+ characters, heavy noise)
```

#### 4.2.3 Label Smoothing
```python
# One-hot encoding ëŒ€ì‹  ë¶€ë“œëŸ¬ìš´ ë ˆì´ë¸” ì‚¬ìš©
loss = CategoricalCrossentropy(label_smoothing=0.1)
```

**ìš°ì„ ìˆœìœ„**: â­â­ Medium

---

### 4.3 Cross-validation

**í˜„ì¬ ìƒíƒœ**: ë‹¨ìˆœ train/validation split (90/10)

**ê°œì„ **:
```python
# K-Fold Cross-validation
from sklearn.model_selection import KFold

kf = KFold(n_splits=5, shuffle=True)
scores = []

for train_idx, val_idx in kf.split(data):
    model = build_model()
    model.fit(data[train_idx], ...)
    score = model.evaluate(data[val_idx])
    scores.append(score)

print(f"Mean accuracy: {np.mean(scores):.3f} Â± {np.std(scores):.3f}")
```

**ìš°ì„ ìˆœìœ„**: â­â­ Medium

---

### 4.4 Hard Negative Mining

**ê°œë…**: ì˜ëª» ì˜ˆì¸¡í•œ ìƒ˜í”Œì„ ë” ìì£¼ í•™ìŠµ

**ë°©ë²•**:
```python
# ì˜ˆì¸¡ ì˜¤ë¥˜ê°€ í° ìƒ˜í”Œì— ê°€ì¤‘ì¹˜ ë¶€ì—¬
sample_weights = compute_sample_weights(predictions, labels)
model.fit(X, y, sample_weight=sample_weights)
```

**ìš°ì„ ìˆœìœ„**: â­â­ Medium

---

## 5. ğŸš€ ìƒˆë¡œìš´ ê¸°ëŠ¥

### 5.1 ë‹¤êµ­ì–´ ìº¡ì±  ì§€ì›

**í˜„ì¬ ìƒíƒœ**: ìˆ«ìë§Œ ì§€ì›

**í™•ì¥**:
- ì˜ë¬¸ ì•ŒíŒŒë²³ (ëŒ€ì†Œë¬¸ì)
- í•œê¸€
- í˜¼í•© (ìˆ«ì + ì˜ë¬¸)

**êµ¬í˜„**:
```python
# ë¬¸ì ì§‘í•© í™•ì¥
CHARACTERS = {
    'digits': '0123456789',
    'alpha': 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz',
    'korean': 'ã„±ã„´ã„·ã„¹ã…ã…‚ã……ã…‡ã…ˆã…Šã…‹ã…Œã…ã…ê°€ë‚˜ë‹¤ë¼ë§ˆë°”ì‚¬ì•„ìì°¨ì¹´íƒ€íŒŒí•˜...',
    'mixed': '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'
}

model = CaptchaModel.load('weights.h5', charset='mixed')
```

**ìš°ì„ ìˆœìœ„**: â­â­â­ High

---

### 5.2 ì‹ ë¢°ë„ ê¸°ë°˜ ì¬ì‹œë„

**ê°œë…**: ë‚®ì€ ì‹ ë¢°ë„ ì˜ˆì¸¡ ì‹œ ìë™ ì¬ì²˜ë¦¬

**êµ¬í˜„**:
```python
result = model.predict('captcha.png', return_confidence=True)

if result.confidence < 0.8:
    # ì „ì²˜ë¦¬ ë³€ê²½ í›„ ì¬ì‹œë„
    result = model.predict(
        'captcha.png',
        preprocessing='aggressive'
    )
```

**ìš°ì„ ìˆœìœ„**: â­â­ Medium

---

### 5.3 ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ìº¡ì±  ì¸ì‹

**ëª©ì **: ì›¹ìº  ë˜ëŠ” ìŠ¤í¬ë¦° ìº¡ì²˜ì—ì„œ ì‹¤ì‹œê°„ ì¸ì‹

**êµ¬í˜„**:
```python
import cv2

cap = cv2.VideoCapture(0)
detector = CaptchaDetector()  # YOLO ê¸°ë°˜ ìº¡ì±  ì˜ì—­ íƒì§€

while True:
    frame = cap.read()
    captcha_region = detector.detect(frame)
    if captcha_region:
        text = model.predict(captcha_region)
        print(f"Detected: {text}")
```

**ìš°ì„ ìˆœìœ„**: â­ Low (íŠ¹ìˆ˜ ìš©ë„)

---

### 5.4 Web API / REST API

**í˜„ì¬ ìƒíƒœ**: Python ë¼ì´ë¸ŒëŸ¬ë¦¬ë§Œ ì œê³µ

**ê°œì„ **:
```python
# FastAPI ê¸°ë°˜ REST API
from fastapi import FastAPI, UploadFile

app = FastAPI()

@app.post("/predict")
async def predict(file: UploadFile):
    image = await file.read()
    result = model.predict_bytes(image)
    return {"text": result, "confidence": 0.95}

# ì‚¬ìš© ì˜ˆì‹œ
curl -X POST -F "file=@captcha.png" http://localhost:8000/predict
```

**ìš°ì„ ìˆœìœ„**: â­â­â­ High (í”„ë¡œë•ì…˜ ë°°í¬ ì‹œ)

---

### 5.5 Browser Extension

**ëª©ì **: ë¸Œë¼ìš°ì €ì—ì„œ ìº¡ì±  ìë™ ì…ë ¥

**êµ¬í˜„**:
- Chrome/Firefox Extension ê°œë°œ
- í˜ì´ì§€ì˜ ìº¡ì±  ì´ë¯¸ì§€ ìë™ ê°ì§€
- ëª¨ë¸ ì¶”ë¡  í›„ ìë™ ì…ë ¥

**ê¸°ìˆ  ìŠ¤íƒ**:
- TensorFlow.js (ë¸Œë¼ìš°ì € ë‚´ ì¶”ë¡ )
- ë˜ëŠ” ì„œë²„ API í˜¸ì¶œ

**ìš°ì„ ìˆœìœ„**: â­ Low (ë³„ë„ í”„ë¡œì íŠ¸)

---

## 6. ğŸŒ í”„ë¡œë•ì…˜ ì¤€ë¹„

### 6.1 ëª¨ë¸ ë²„ì „ ê´€ë¦¬

**êµ¬í˜„**:
```python
# ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬
from captcha_cracker import ModelRegistry

registry = ModelRegistry('s3://models/')
registry.register(
    model_path='weights_v3.h5',
    version='1.2.0',
    metrics={'accuracy': 0.98, 'speed': '20ms'},
    tags=['production', 'numbers-only']
)

# í”„ë¡œë•ì…˜ì—ì„œ ìµœì‹  ëª¨ë¸ ìë™ ë¡œë“œ
model = CaptchaModel.load_from_registry(
    stage='production',
    version='latest'
)
```

**ë„êµ¬**: MLflow, DVC, Weights & Biases

**ìš°ì„ ìˆœìœ„**: â­â­â­ High

---

### 6.2 ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

**êµ¬í˜„**:
```python
# ì˜ˆì¸¡ ê²°ê³¼ ë¡œê¹…
import logging
from prometheus_client import Counter, Histogram

prediction_counter = Counter('predictions_total', 'Total predictions')
prediction_latency = Histogram('prediction_latency_seconds', 'Prediction latency')

@prediction_latency.time()
def predict(image):
    result = model.predict(image)
    prediction_counter.inc()
    logger.info(f"Predicted: {result}, confidence: {result.confidence}")
    return result
```

**ë©”íŠ¸ë¦­**:
- ì˜ˆì¸¡ íšŸìˆ˜
- í‰ê·  ì‘ë‹µ ì‹œê°„
- ì‹ ë¢°ë„ ë¶„í¬
- ì˜¤ë¥˜ìœ¨

**ìš°ì„ ìˆœìœ„**: â­â­â­ High (í”„ë¡œë•ì…˜ ì‹œ)

---

### 6.3 A/B í…ŒìŠ¤íŒ…

**ëª©ì **: ìƒˆ ëª¨ë¸ê³¼ ê¸°ì¡´ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ

**êµ¬í˜„**:
```python
# Traffic splitting
if random.random() < 0.1:  # 10% íŠ¸ë˜í”½
    result = model_v2.predict(image)
else:
    result = model_v1.predict(image)

# ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ë¹„êµ
collect_metrics(model_version, result, ground_truth)
```

**ìš°ì„ ìˆœìœ„**: â­â­ Medium

---

### 6.4 ìºì‹±

**ëª©ì **: ë™ì¼ ì´ë¯¸ì§€ ì¬ì˜ˆì¸¡ ë°©ì§€

**êµ¬í˜„**:
```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=1000)
def predict_cached(image_hash: str):
    return model.predict(image_hash)

# ì‚¬ìš©
image_hash = hashlib.md5(image_bytes).hexdigest()
result = predict_cached(image_hash)
```

**ìš°ì„ ìˆœìœ„**: â­â­ Medium

---

### 6.5 Rate Limiting

**ëª©ì **: API ë‚¨ìš© ë°©ì§€

**êµ¬í˜„**:
```python
from slowapi import Limiter

limiter = Limiter(key_func=get_remote_address)

@app.post("/predict")
@limiter.limit("10/minute")  # ë¶„ë‹¹ 10íšŒ ì œí•œ
async def predict(file: UploadFile):
    ...
```

**ìš°ì„ ìˆœìœ„**: â­â­â­ High (API ì œê³µ ì‹œ)

---

## 7. ğŸ”¬ ì—°êµ¬ ë° ì‹¤í—˜

### 7.1 Self-Supervised Learning

**ê°œë…**: ë ˆì´ë¸” ì—†ëŠ” ë°ì´í„°ë¡œ ì‚¬ì „ í•™ìŠµ

**ë°©ë²•**:
```python
# Contrastive Learning (SimCLR)
# ë™ì¼ ì´ë¯¸ì§€ì˜ ë‹¤ë¥¸ augmentationì„ ê°€ê¹ê²Œ í•™ìŠµ
loss = contrastive_loss(
    embedding1=encoder(augment1(image)),
    embedding2=encoder(augment2(image))
)
```

**ì˜ˆìƒ íš¨ê³¼**:
- ì ì€ ë ˆì´ë¸” ë°ì´í„°ë¡œ ë†’ì€ ì„±ëŠ¥
- ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ

**ìš°ì„ ìˆœìœ„**: â­ Low (ì—°êµ¬ ë‹¨ê³„)

---

### 7.2 Few-Shot Learning

**ê°œë…**: ì ì€ ìƒ˜í”Œë¡œ ìƒˆë¡œìš´ ìº¡ì±  íƒ€ì… í•™ìŠµ

**ë°©ë²•**:
- Meta-Learning (MAML, Prototypical Networks)
- 5-10ê°œ ìƒ˜í”Œë¡œ ìƒˆë¡œìš´ ìº¡ì±  ìŠ¤íƒ€ì¼ ì ì‘

**ìš°ì„ ìˆœìœ„**: â­ Low (ì—°êµ¬ ë‹¨ê³„)

---

### 7.3 Adversarial Training

**ëª©ì **: ì ëŒ€ì  ê³µê²©ì— ê°•ê±´í•œ ëª¨ë¸

**ë°©ë²•**:
```python
# FGSM (Fast Gradient Sign Method)
adversarial_images = generate_adversarial_examples(images)
model.train_on_batch(adversarial_images, labels)
```

**ìš°ì„ ìˆœìœ„**: â­ Low

---

### 7.4 Neural Architecture Search (NAS)

**ëª©ì **: ìµœì  ëª¨ë¸ êµ¬ì¡° ìë™ íƒìƒ‰

**ë„êµ¬**: AutoKeras, KerasTuner

**ìš°ì„ ìˆœìœ„**: â­ Low (ì—°êµ¬ ë‹¨ê³„)

---

## ìš°ì„ ìˆœìœ„ ë§¤íŠ¸ë¦­ìŠ¤

| ê°œì„  í•­ëª© | ì˜í–¥ë„ | êµ¬í˜„ ë‚œì´ë„ | ìš°ì„ ìˆœìœ„ | ì˜ˆìƒ ê¸°ê°„ |
|-----------|--------|-------------|----------|-----------|
| ë°ì´í„° ì¦ê°• | High | Low | â­â­â­ | 1ì£¼ |
| ONNX ë³€í™˜ | High | Medium | â­â­â­ | 2ì£¼ |
| ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™” | High | Low | â­â­â­ | 1ì£¼ |
| ëª¨ë¸ ì–‘ìí™” | High | Medium | â­â­â­ | 2ì£¼ |
| í•©ì„± ë°ì´í„° ìƒì„± | High | Medium | â­â­â­ | 2ì£¼ |
| ë‹¤êµ­ì–´ ì§€ì› | Medium | High | â­â­â­ | 3ì£¼ |
| Web API | High | Low | â­â­â­ | 1ì£¼ |
| ëª¨ë¸ ë²„ì „ ê´€ë¦¬ | Medium | Medium | â­â­â­ | 2ì£¼ |
| Attention Mechanism | High | High | â­â­â­ | 3ì£¼ |
| Monitoring/Logging | Medium | Low | â­â­â­ | 1ì£¼ |
| Mixed Precision | Medium | Low | â­â­â­ | 1ì£¼ |
| CNN Backbone ì—…ê·¸ë ˆì´ë“œ | Medium | High | â­â­ | 3ì£¼ |
| ì•™ìƒë¸” | Low | Low | â­â­ | 1ì£¼ |
| ì§€ì‹ ì¦ë¥˜ | Medium | High | â­â­ | 3ì£¼ |
| Learning Rate Scheduler | Low | Low | â­â­ | 3ì¼ |

---

## ì œì•ˆ ë¡œë“œë§µ

### Phase 1 (v1.1.0) - Quick Wins (2-3ì£¼)
**ëª©í‘œ**: ë¹ ë¥´ê²Œ ì ìš© ê°€ëŠ¥í•œ ê°œì„ 
- [ ] ë°ì´í„° ì¦ê°•
- [ ] ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
- [ ] Mixed Precision í•™ìŠµ
- [ ] í•©ì„± ë°ì´í„° ìƒì„±
- [ ] Learning Rate Scheduler

**ì˜ˆìƒ íš¨ê³¼**:
- ì •í™•ë„: 3-5% í–¥ìƒ
- í•™ìŠµ ì†ë„: 2ë°° í–¥ìƒ

---

### Phase 2 (v1.2.0) - ì„±ëŠ¥ ìµœì í™” (3-4ì£¼)
**ëª©í‘œ**: ì¶”ë¡  ì†ë„ ë° ê²½ëŸ‰í™”
- [ ] ëª¨ë¸ ì–‘ìí™”
- [ ] ONNX ë³€í™˜
- [ ] ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ìµœì í™”
- [ ] ëª¨ë¸ ë²„ì „ ê´€ë¦¬

**ì˜ˆìƒ íš¨ê³¼**:
- ì¶”ë¡  ì†ë„: 2-4ë°° í–¥ìƒ
- ëª¨ë¸ í¬ê¸°: 75% ê°ì†Œ

---

### Phase 3 (v1.3.0) - ì•„í‚¤í…ì²˜ ê°œì„  (4-6ì£¼)
**ëª©í‘œ**: ëª¨ë¸ êµ¬ì¡° í˜„ëŒ€í™”
- [ ] Attention Mechanism ì¶”ê°€
- [ ] CNN Backbone ì—…ê·¸ë ˆì´ë“œ (ResNet ë˜ëŠ” EfficientNet)
- [ ] Dynamic Image Size ì§€ì›

**ì˜ˆìƒ íš¨ê³¼**:
- ì •í™•ë„: 5-7% í–¥ìƒ
- ë‹¤ì–‘í•œ ìº¡ì±  íƒ€ì… ì§€ì›

---

### Phase 4 (v1.4.0) - í”„ë¡œë•ì…˜ ì¤€ë¹„ (2-3ì£¼)
**ëª©í‘œ**: ì‹¤ì œ ì„œë¹„ìŠ¤ ë°°í¬
- [ ] Web API (FastAPI)
- [ ] ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…
- [ ] Rate Limiting
- [ ] ìºì‹±
- [ ] A/B í…ŒìŠ¤íŒ…

**ì˜ˆìƒ íš¨ê³¼**:
- í”„ë¡œë•ì…˜ ì¤€ë¹„ ì™„ë£Œ
- ì•ˆì •ì ì¸ ì„œë¹„ìŠ¤ ìš´ì˜

---

### Phase 5 (v2.0.0) - ë‹¤êµ­ì–´ ë° ê³ ê¸‰ ê¸°ëŠ¥ (6-8ì£¼)
**ëª©í‘œ**: ê¸°ëŠ¥ í™•ì¥
- [ ] ë‹¤êµ­ì–´ ìº¡ì±  ì§€ì›
- [ ] ì•™ìƒë¸” ëª¨ë¸
- [ ] ì‹ ë¢°ë„ ê¸°ë°˜ ì¬ì‹œë„
- [ ] ì§€ì‹ ì¦ë¥˜

**ì˜ˆìƒ íš¨ê³¼**:
- ì˜ë¬¸, í•œê¸€ ë“± ë‹¤ì–‘í•œ ìº¡ì±  ì§€ì›
- ìµœê³  ìˆ˜ì¤€ì˜ ì •í™•ë„

---

## ì‹¤í—˜ ì¶”ì 

### ì‹¤í—˜ í…œí”Œë¦¿
```markdown
## ì‹¤í—˜ #XX: [ì œëª©]

**ë‚ ì§œ**: YYYY-MM-DD
**ëª©ì **: [ì‹¤í—˜ ëª©ì ]
**ê°€ì„¤**: [ê²€ì¦í•˜ë ¤ëŠ” ê°€ì„¤]

### ì‹¤í—˜ ì„¤ì •
- ë°ì´í„°ì…‹: [ì‚¬ìš©í•œ ë°ì´í„°]
- ëª¨ë¸: [ëª¨ë¸ ì•„í‚¤í…ì²˜]
- í•˜ì´í¼íŒŒë¼ë¯¸í„°:
  - Learning rate: 1e-3
  - Batch size: 16
  - Epochs: 100

### ê²°ê³¼
- Baseline ì •í™•ë„: 92.5%
- ê°œì„  í›„ ì •í™•ë„: 95.3% (+2.8%)
- ì¶”ë¡  ì†ë„: 15ms â†’ 12ms (20% í–¥ìƒ)

### ë¶„ì„
[ê²°ê³¼ ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸]

### ê²°ë¡ 
- [ ] í”„ë¡œë•ì…˜ ì ìš©
- [ ] ì¶”ê°€ ì‹¤í—˜ í•„ìš”
- [ ] ê¸°ê°

### ë‹¤ìŒ ë‹¨ê³„
[í›„ì† ì‹¤í—˜ ë˜ëŠ” ê°œì„  ë°©í–¥]
```

---

## ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (ëª©í‘œ)

| ë©”íŠ¸ë¦­ | í˜„ì¬ (v1.0) | ëª©í‘œ (v2.0) | ê°œì„ ìœ¨ |
|--------|-------------|-------------|--------|
| ì •í™•ë„ | 92% | 97%+ | +5%p |
| ì¶”ë¡  ì†ë„ (CPU) | 50ms | 15ms | 3.3ë°° |
| ì¶”ë¡  ì†ë„ (GPU) | 10ms | 3ms | 3.3ë°° |
| ëª¨ë¸ í¬ê¸° | 20MB | 5MB | 75% ê°ì†Œ |
| ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | 500MB | 200MB | 60% ê°ì†Œ |

---

## ì°¸ê³  ìë£Œ

### ë…¼ë¬¸
- **Attention**: "Attention Is All You Need" (Vaswani et al., 2017)
- **CTC**: "Connectionist Temporal Classification" (Graves et al., 2006)
- **Data Augmentation**: "A survey on Image Data Augmentation for Deep Learning" (Shorten & Khoshgoftaar, 2019)
- **Knowledge Distillation**: "Distilling the Knowledge in a Neural Network" (Hinton et al., 2015)

### ë¸”ë¡œê·¸ & íŠœí† ë¦¬ì–¼
- TensorFlow Model Optimization Toolkit
- ONNX Runtime Best Practices
- Mixed Precision Training Guide

### ê´€ë ¨ í”„ë¡œì íŠ¸
- Tesseract OCR
- EasyOCR
- PaddleOCR

---

## ê¸°ì—¬ ê°€ì´ë“œ

ì»¤ë®¤ë‹ˆí‹° ê¸°ì—¬ë¥¼ í™˜ì˜í•©ë‹ˆë‹¤! ë‹¤ìŒ í•­ëª©ì— ê¸°ì—¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. **ìƒˆë¡œìš´ ê°œì„  ì•„ì´ë””ì–´ ì œì•ˆ**
   - GitHub Issueì— ì œì•ˆì„œ ì‘ì„±
   - ì‹¤í—˜ ê²°ê³¼ ê³µìœ 

2. **ì‹¤í—˜ ê²°ê³¼ ì œì¶œ**
   - ìœ„ ì‹¤í—˜ í…œí”Œë¦¿ ì‚¬ìš©
   - Pull Requestë¡œ ë¬¸ì„œ ì—…ë°ì´íŠ¸

3. **ìƒˆë¡œìš´ ìº¡ì±  ë°ì´í„°ì…‹ ê³µìœ **
   - ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼ì˜ ìº¡ì±  ì´ë¯¸ì§€
   - ë ˆì´ë¸”ë§ëœ ë°ì´í„°

---

**ë¬¸ì„œ ë²„ì „**: 1.0
**ìµœì¢… ìˆ˜ì •**: 2025-11-19
**ìƒíƒœ**: ğŸŸ¢ ê²€í†  ì¤‘ (Backlog)

**Note**: ë³¸ ë¬¸ì„œëŠ” ì‚´ì•„ìˆëŠ” ë¬¸ì„œ(Living Document)ì…ë‹ˆë‹¤.
ì‹¤í—˜ ê²°ê³¼ì™€ ì»¤ë®¤ë‹ˆí‹° í”¼ë“œë°±ì— ë”°ë¼ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.
